# -*- coding: utf-8 -*-
"""Data_Science.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gLetWuyRRnoGkQNGgB-7dy2m9yNPMjOM
"""

# @title Default title text
import pandas as pd
import seaborn as sns
link = pd.read_csv('/content/linkedin-reviews.csv')
print(link)

# Explaoratory Data Analysis
# We will start analyazing the disbtributions of ratings . it will provide insight into the overall
# sentiment of the reviews . Then we can futher such as analyazing the length of reviews
#  and possibly dervie insisghts from the text of the reviews


#Plotting the Distribution of Ratings
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns




sns.set(style="whitegrid")
plt.figure(figsize=(9,5))
sns.countplot(data = link , x = 'Rating')
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

"""# Adding Sentiment Labels in the Data

We will use Textblob libary. Textblob provides a polarity scores raning from -1(very negatiive) to 1 (very postive) fro a given text . We can use this score to classify each review's sentiment as psostive , negaative or netural .
"""

from textblob import TextBlob
!pip install textblob

def TextBlob_sentiment_analysis(review):
    sentiment = TextBlob(review).sentiment
    if sentiment.polarity > 0.1:
        return 'Positive'
    elif sentiment.polarity < -0.1:
        return 'Negative'
    else:
        return 'Neutral'

link['Sentiment'] = link['Review'].apply(TextBlob_sentiment_analysis)

link.sample(5)

"""# Analyzing App Reviews Sentiments

"""

Sentiment_distribution = link['Sentiment'].value_counts()
print(Sentiment_distribution)

!pip install seaborn
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(9,5))
sns.barplot(x= Sentiment_distribution.index , y = Sentiment_distribution.values)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

"""# **So we can see although the app has low ratings , still the reviewers don't use many negative words in the reviews for thr app**

Next,  we'll explore thre realtionship between the sentiments and the ratings . This analysis can help us undersdtand there is a corealtion b/w the sentimenet of the trxt and numerical ratings.
"""

plt.figure(figsize=(10,5))
sns.countplot(data = link , x = 'Rating' , hue = 'Sentiment')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.legend(title = 'Sentiment')
plt.show()

import pandas as pd
import numpy as np


df = pd.read_csv("/content/placement.csv")
df.head(3)

df.isnull().sum()

df.shape

x = df.drop(columns = ['placed'] , axis = 1)   ## Input columns
y = df['placed']    ## target column

# @title Default title text
print(x.shape)
print(y.shape)

from sklearn.model_selection import train_test_split



x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42) # Split data into training and testing sets

print(x_train.shape)
print(y_test.shape)
print(x_test.shape)
print(y_train.shape)

"""# Standardrization"""

np.round(x_train.describe(),2)

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

x_train_sc = sc.fit_transform(x_train)

x_train_new = pd.DataFrame(x_train_sc , columns = x.columns)

x_train_new.head(3)

"""# ***Normalization***"""

from sklearn.preprocessing import MinMaxScaler

mn = MinMaxScaler()

x_train_mn = mn.fit_transform(x_train)

x_train_new = pd.DataFrame(x_train_mn , columns = x.columns)

np.round(x_train_new.describe(),2)

"""# **Encoding ===> Convert categirical data into numerical data ***"""

df = pd.read_csv("/content/tips.csv")

df.head(3)

df['sex'].value_counts()df

df['smoker'].value_counts()

df['day'].value_counts()

df['time'].value_counts()

df.isnull().sum()

from sklearn.preprocessing import LabelEncoder

lb = LabelEncoder()

df['sex'] = lb.fit_transform(df['sex'])
df['smoker'] = lb.fit_transform(df['smoker'])
df['day'] = lb.fit_transform(df['day'])
df['time'] = lb.fit_transform(df['time'])

df.head(3)

x = df.drop(columns = ['total_bill'] , axis = 1)   ## Input columns
y = df['total_bill']    ## target column

from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42) # Split data into training and testing sets

print("Toal data shape : ", df.shape)
print("Independant data shape : ", x.shape)
print("Dependent data shape : ", y.shape)
print("x_train data shape : ", x_train.shape)
print("x_test data shape : ", x_test.shape)
print("y_train data shape : ", y_train.shape)
print("y_test data shape : ", y_test.shape)

"""# **Standardrization**"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

x_train_sc = sc.fit_transform(x_train)

x_train_new = pd.DataFrame(x_train_sc , columns = x.columns)

np.round(x_train_new.describe(),2)

"""# **Normalization**"""

from sklearn.preprocessing import MinMaxScaler

mn = MinMaxScaler()

x_train_mn = mn.fit_transform(x_train)

x_train_new = pd.DataFrame(x_train_mn , columns = x.columns)

np.round(x_train_new.describe(),2)

df = pd.read_csv("/content/Social_Network_Ads.csv")

df.head(3)

df['User ID'].value_counts()

df['EstimatedSalary'].value_counts()

df['Age'].value_counts()

df['Gender'].value_counts()

df['Purchased'].value_counts()

df.isnull().sum()

from sklearn.preprocessing import LabelEncoder

lb = LabelEncoder()

df['User ID'] = lb.fit_transform(df['User ID'])
df['EstimatedSalary'] = lb.fit_transform(df['EstimatedSalary'])
df['Age'] = lb.fit_transform(df['Age'])
df['Gender'] = lb.fit_transform(df['Gender'])
df['Purchased'] = lb.fit_transform(df['Purchased'])

df.head(3)

x = df.drop(columns = ['User ID'] , axis = 1)   ## Input columns
y = df['User ID']    ## target column

from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42) # Split data into training and testing sets

print("Toal data shape : ", df.shape)
print("Independant data shape : ", x.shape)
print("Dependent data shape : ", y.shape)
print("x_train data shape : ", x_train.shape)
print("x_test data shape : ", x_test.shape)
print("y_train data shape : ", y_train.shape)
print("y_test data shape : ", y_test.shape)

"""# **Standardriazation**"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

x_train_sc = sc.fit_transform(x_train)

x_train_new = pd.DataFrame(x_train_sc , columns = x.columns)

np.round(x_train_new.describe(),2)

"""# **Normalization **"""

from sklearn.preprocessing import MinMaxScaler

mn = MinMaxScaler()

x_train_mn = mn.fit_transform(x_train)

x_train_new = pd.DataFrame(x_train_mn , columns = x.columns)

np.round(x_train_new.describe(),2)

df = pd.read_csv("/content/covid_toy.csv")

df.isnull().sum()

x = df.dropna()

df.isnull().sum()

df.head(3)

df['gender'].value_counts()

df['cough'].value_counts()

df['city'].value_counts()

df['has_covid'].value_counts()

from sklearn.preprocessing import LabelEncoder

lb = LabelEncoder()

df['gender'] = lb.fit_transform(df['gender'])
df['cough'] = lb.fit_transform(df['cough'])
df['city'] = lb.fit_transform(df['city'])
df['has_covid'] = lb.fit_transform(df['has_covid'])

x = df.drop(columns = ['gender'] , axis = 1)   ## Input columns
y = df['gender']    ## target column

from sklearn.model_selection import train_test_split



x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42) # Split data into training and testing sets

print("Toal data shape : ", df.shape)
print("Independant data shape : ", x.shape)
print("Dependent data shape : ", y.shape)
print("x_train data shape : ", x_train.shape)
print("x_test data shape : ", x_test.shape)
print("y_train data shape : ", y_train.shape)
print("y_test data shape : ", y_test.shape)

"""# **Standardrization**"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

x_train_sc = sc.fit_transform(x_train)

x_train_new = pd.DataFrame(x_train_sc , columns = x.columns)

np.round(x_train_new.describe(),2)

